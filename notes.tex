\documentclass[12pt,letterpaper]{report}
\usepackage[utf8x]{inputenc}
\usepackage{ucs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Michael Kummer}
\begin{document}

Text: An Introduction to Compressive Sensing.
Collection Editors:
Richard Baraniuk
Mark A. Davenport
Marco F. Duarte
Chinmay Hegde

Parseval Theorem

Parseval Frames

Frame Operator

	Matrix whose rows are transposed vectors

Moore-Penrose Pseudoinverse

	$Q^TQx = Q^Ta$

	$x = (Q^TQ)^{-1}Q^Ta = Q^\dagger a$

$D = (Q^TQ)^{-1}Q^T$

reproducing formula

$\sum a_n d_n = \sum <x, \phi_n>d_n = x$

\begin{enumerate}
\item Statistical learning theory  - avoid overfitting
\item Statistical estimation - Model selection
\item Neural information processing - visual perception
\end{enumerate}

Error $||x - \hat{x}||^2 = \sum^\infty_{n=k}|<x, v_n>|^2$

$|a_n - \hat{a_n}| = \{ 0 : n < k, a_n : n \geq k\}$

$dctmtx(4)$

projection pursuit algorithms


\subsection*{9-19-2016}

$\min \sum |a_n|, Da = x$

$d = 1, x \in \mathbb{R}$

Start with a ball at the center, once it touches the hyperplane, that's the least squares solution.  With L1 norm, diamond.

$v_n(i) = cos(ni\pi /N) = e^{-j*2\pi (n-1)(j-1)/d}$

\subsection*{9-26-2016}

Single pixel camera

$y \in \mathbb{R}^m, x \in \mathbb{R}^m, \Phi \in \mathbb{R}^{m \times n}, m < n$

$y = Ax + e$

$A_{ij}$ represents variables that impact the outcome.

A = design prediction matrix

blood pressure, weight, height, age, gene expression levels, ...

The output $y_i$ is the probability that patient i suffers from a certain disease

m patients, n variables

$y = f(t) + e$

Error correction $[z]_{10}$ would like to transmit over a communication channel

$v = B z$

$v \rightarrow$ receive $v + x$, $||x||_0 < s$

Checksum matrix A such that $AB=0$

$y = Aw = A(v+x) = ABz + Ax = Ax$

Solve $y = Ax$ subject to $||x||_0 < s$

\subsection*{9-28-2016}
$y = \Phi x, y \in \mathbb{R}^m, x \in \mathbb{R}^n, m < n$

Learning for sparse solutions

xray ct, mri, look at an image in an xray scanner (square), xray scanner has a linear accelerator that shoots beams of xrays through a biological material. As they travel through, they hit softer and denser points in the space.  Denser materials scatter more of the rays.  A particular measurement say $y_1$ is the inner product of the image we'd like to recover and a vector 1 along the ray and 0 elsewhere (basis images $\phi_1$.  As few measurements as possible while solving the linear system $y = \Phi x$. $y_1 = < \Box, f(\Box) >$ (fourier box of parallel diagonal lines).

QM physicists went to applied Radar problems.  These techniques allow an increase in resolution beyond what was previously thought possible.

When can we hope to recover x from y?  $x \in A \mathbb{R}^n, y \in B \mathbb{R}^m, \Phi : x -> y$  In general there are infinitely many points that produce y observations.  If $x_0$ is one of them, then we have $y = \Phi x_0, y = \Phi (x_0 + \alpha * v), v \in null(\Phi),  \alpha \in \mathbb{R}$, Now suppose x is... $T = \{ 3,5,8 \}$  $T = \{j : x_j \neq 0 \}$ support, If we knew T, $y = \Phi x = \Phi_T x_T$ (3,5,8 columns).  $m > s$ If $\Phi_T$ is full rank then $\Phi_T^t \Phi_T$ is invertible (it's minimum eigen value is larger than 0).  We can recover $x_T$ from y we can find x.

We want $\lambda_{min} > const > 0$ for reliability reasons.  Stable recovery requires that all eigenvalues are close to 1.  In other words if we multiply $\Phi_T^t \Phi_T c \approx c$ this is preserving the length of the eigenvectors.  $(1 - s) ||c||_2^2 \leq ||\Phi_T c||_2^2 \leq (1 + \delta)||c||_2^2, 1 > \delta > 0$, $(1 - s) \leq \lambda_{max}(\Phi_T^t \Phi_T) < (1 + \delta)$.  Now if we don't know T: When will y be different for ALL x with $||x||_0 \leq s^2$? (We want a condition on $\Phi$)

If we have two s-sparse vectors $x_1, x_2, ||x_1||_0 \leq s, ||x_2||_0 \leq s, y = \Phi x_1 = \Phi x_2, \Phi x_1 - \Phi x_2 = 0 \rightarrow \Phi (x_1 - x_2) = 0, ||x_1 - x_2||_0 \leq ||x_1||_0 + ||x_2||_1$, $x' = x_1 - x_2, ||x'||_0 \leq 2s, \Phi x' = 0, \rightarrow ||\Phi x'||_2 = 0$,  This means that $\exists \Phi_T$ with $|T| = 2S$ that $\Phi_T$ is singular, $\lambda_{min}(\Phi_T^t \Phi_T) = 0$

1) For two s sparse vectors to have the same image (y) a 2s sparse vector must be in $null(\Phi)$, $\exists x_1' ||x'||_0 \leq 2s, ||\Phi x'|| = 0$

\subsection*{9-30-2016}

Talk about the homework, 1st problem was ok, 2nd some people went too quickly to google.  I really wanted you to understand the geometry.  Everyone solved the first correctly which meant you understood it, but the second problem was to apply your understanding.  I strongly instruct you to look at the second problem in light of the first.  The concept of a vector being something very general should be something you feel comfortable with.  The second assignment you'll be dealing with nonlinear approximations.  If you haven't fully developed that understanding of vectors in the first place the second assignment won't be as meaningful.  Take another look at the 2nd problem.

It's about time for you to be looking into projects.  I've recommended some papers to some who have asked, some of you haven't asked.  We'll have a proposal deadline in October and you'll pick a particular problem in some special topic of interest.  You'll prepare a presentation on that subject and you'll implement an algorithm or two on that topic and we'll see how that works.  You'll search terms like dictionary representations, sparse representations, etc.  Google is your friend here (not for solving your assignments).  Start looking to see what interests you.  There are a few posts on canvas that give you some directions.  Once you've done that please come talk to me.  I'll help you narrow down what you're looking for so you can get to a presentation.  You need to do some initial research first.  Spend time looking for time looking for problems that interest you.

We have seen some results with spikes and fourier dictionary.  We're extending our understanding of linear algebra (LA) to see when we can solve an under determined linear system.  LA says you can't, when can you come to a conclusion that you can solve linear system with a sparse solution?  (Google sparse solutions to linear systems)

$y = \Phi x, y \in \mathbb{R}^m, x \in \mathbb{R}^n, m < n$

$x \in \mathbb{R}^n, \phi(x) = y \in \mathbb{R}^m$  (Mapping with sets and arrows)

Q: When the preimage of y (among sparse vectors) is unique.  A: For two sparse vectors to have the same image, a 2s-sparse vector must be in $Null(\Phi), \Phi(x_1 - x_2) = 0, x' := (x_1 - x_2)$,

Every s-sparse vector has an image (y) which is unique (only come from x) if

$0 < ||\Phi x' ||_2^2, \forall ||x'||_0 \leq 2s$

For all 2s-sparse vectors then we can recover any s-sparse x from $y = \Phi x$ as x will be the unique solution to: $\min_z||z||_0 Such that \Phi z = y$.

Ex: $n = 100, m = 50,  y_m = A_{m \times n} x_n$ if only 5 of x are non zero, is there only 1 that gives this sparse solution?  Giving 50 measurements (y), where are those zeros at?  We're about to look at some proofs, but this part should be absolutely clear to everyone.  $|| \Phi x' ||_2^2 > 0 if ||x'|| \leq 10$

Student: How do we come up with this matrix?  Teacher: That's a very good question, I hope to get to that.

To make recovery (of x from y) stable we need this very important inequality (a key property of $\Phi$): $(1 - \delta) ||x'||_2^2 \leq ||\Phi x||_2^2 \leq (1 + \delta) ||x'||_2^2$ For ALL 2S-sparse vectors $x'$.  $0 < \delta < 1$, if $\delta = \frac{1}{2}: \frac{1}{2}||x'||_2^2 \leq ||\Phi x||_2^2 \leq \frac{3}{2}||x'||_2^2$

If this property holds for a  matrix, we say $\Phi$ has the restricted isometry property (RIP) of size 2S and constant $\delta$. $\Phi$ 2S-RIP or 2S-RIP-$\delta$.

$(1-\delta)||x||_2^2 < ||\Phi x||_2^2 < (1+\delta)||x||_2^2$... isometry talk, 3d vectors from 3 to 2, the length corresponds from 3d to 2d.  It's off by $\delta$.

What matrices have this RIP property?  Here's where to invoke the gold coin problem.  We had 7 coins and wanted to know the fewest number of measurements (y) and we wanted to build a $\Phi$ and determine which coin was defective with the fewest number of measurements.  $m=3, n=7, y_m = \Phi_{m \times n} x_n$ We saw a solution for 3 measurements, s is 1 (one defective coin). How do we solve the gold coin problem in a more general setting?  Instead of sparsity 1 what if I give you sparsity s, how many measurements would you need? $m \approx s log(n/s), m > 2s$.  (See last lecture for explanation $\Phi_T^t \Phi_T)$

Next class, we can't build these $\Phi$ matrices, we don't have a deterministic algorithm to give you $\Phi$.  Instead if we generate $\Phi_{ij} ~ N(0, 1/m)$ (from some normal distribution, randomness is our friend here).

\subsection*{9-30-2016}
The other day we learned about the Restricted Isometry Property (RIP). $(1 - \delta) ||x'||_2^2 \leq ||\Phi x||_2^2 \leq (1 + \delta) ||x'||_2^2$ for ALL 2s-sparse vectors $x' \in \mathbb{R}^n$.  Remember $y = \Phi x, x \in \mathbb{R}^n, y \in \mathbb{R}^m$.  Then we can say that we can recover any s-sparse x from y.

We are now after solving the gold coin problem in a more general setting.  Holy grail: How small can m be (at least 2s)? $\Phi_{ij} ~ \mathcal{N}(0, 1/m)$  Then $\Phi$ will have 2S-RIP with high probability of $m > const*S*log(n/s)$.  Gold Coin problem: $s=1, n=7, m=3 ~ S*log(n/s)$. To show this, we need to show $||\Phi x|| \approx ||x||$ for all 2S-Sparse vectors.  Tiny example (drawing a mapping from 3D to 2D): $n = 3, m = 2, \phi : x \in X \rightarrow y \in Y$, $\Phi_{ij} ~ \mathcal{N}(0, 1/m), []_{2 \times 3}$, length of $y = ||\Phi x||$

1. Fix x: concentration of measure $||\Phi x||_2^2 \approx ||x||_2^2$ with extraordinarily high probability.

2. For all 2S-sparse vectors (How many do we have?) $||\Phi x||_2^2 \approx ||x||_2^2$ holds with slightly smaller probability.

Fix x, $y = \Phi x, y (rand), \Phi (rand), x (fix)$ then

1. $E[||y||^2] = ||x||_2^2$

2. $pr(| ||y||_2^2 - ||x||_2^2 | > \delta ||x||_2^2) \leq 2*e^{-(\frac{\delta^2}{2} -\frac{\delta^3}{3})m/2}$

$y_i = \Sigma_{j=1}^n \Phi_{ij} x_j, x_j (fixed), \Phi_{ij} (i.i.d. Gaussian), y_i, 1 \leq i \leq m$ are i.i.d Gaussians.  $Var(y_i) = \Sigma_{j=1}^n Var(\Phi_{ij}) x_j^2 = \Sigma \frac{1}{m}x_j^2 = \frac{1}{m}||x||_2^2. y_i ~ \mathcal{N}(1, \frac{1}{m}||x||_2^2$.  $E||y||^2 = \Sigma_{i=1}^m E|y_i|^2 = \Sigma \frac{1}{m}||x||^2$  Remember Markov Inequality: $Pr(Y > \alpha) \leq \frac{E(Y)}{\alpha}$ (positive Y).  $E(Y) = \int_0^\infty y *f_Y(y)dy \geq \int_\alpha^\infty y *f_Y(y)dy \geq \alpha \int_\alpha^\infty f_Y(y)dy = \alpha Pr(Y>\alpha)$

Let $Y = ||y||_2^2$ and assume $||x||_2^2 = 1$, then  $Pr(Y > 1 + \delta) = Pr(\lambda Y > \lambda(1+\delta)) = Pr(e^{\lambda Y} > e^{\lambda(1 + \delta)})$, Invoke Markov: $pr(Y > 1 + \delta) \leq \frac{E[e^{\lambda Y}]}{e^{\lambda(1+\delta)}}$

$E[e^{\lambda Y}] = E[e^{\lambda \Sigma_{i=1}^m y_i^2}] = E[e^{\lambda y_1^2}e^{\lambda y_2^2}...e^{\lambda y_m^2}]$ $y_i$'s are i.i.d Gaussians.

$E[e^{\lambda Y}] = (E[e^{\lambda y_1^2}])^m, y_1 ~ \mathcal{N}(0, 1/m)$ (pdf)

$E[e^{\lambda y_1^2}] = \int e^{\lambda y_1^2} \sqrt{\frac{m}{2 \pi}} e^{-y_1^2*m/2} dy_1 = \int \sqrt{\frac{m}{2 \pi}} e^{-y_1^2(m-2\lambda)/2} dy_1$

$=\sqrt{\frac{m}{m - 2\lambda}} \int \sqrt{\frac{m - 2\lambda}{2 \pi}} e^{-y_1^2(m-2\lambda)/2} dy_2$

$=\sqrt{\frac{m}{m - 2\lambda}} = (1 - 2 \lambda/m)^{1/2} \qquad$ if $\lambda < \frac{m}{2}$

$ E[e^{\lambda y_1^2}] = (1 - 2 \frac{\lambda}{m})^{-1/2} \rightarrow E[e^{\lambda Y}] = (1 - 2 \frac{\lambda}{m})^{-m/2}$

$Pr(Y > 1 + \delta) \leq \frac{(1 - 2 \lambda / m)^{-m/2}}{e^{\lambda(1+\delta)}}$

$=(\frac{e^{-2 \lambda / m(1+\delta)}}{(1 - 2 \lambda / m)})^{m/2} \qquad \forall \lambda < m/2$


Choose $\lambda = \frac{m \delta}{2(1 + \delta)}$

$pr(Y > 1 + \delta) \leq (\frac{e^{-\delta}}{1 - \frac{\delta}{1+\delta}})^{m/2}$

$\log(1 + \delta) \approx \delta - \frac{\delta^2}{2} + \frac{\delta^3}{3} \rightarrow 1 + \delta < e^{\delta - \frac{\delta^2}{2} + \frac{\delta^3}{3}}$


$Pr(||y||_2^2 \geq (1 + \delta) ||x_2^2) < e^{-(\frac{\delta^2}{2} + \frac{\delta^3}{3})m/2}$

$Pr(||y||_2^2 < (1 - \delta) ||x_2^2) < e^{-(\frac{\delta^2}{2} + \frac{\delta^3}{3})m/2}$

$* ||\Phi x||_2^2 \approx ||x||_2^2, (y = \Phi x)$

Example:

Say $\delta = 1/2, m = 1000. Pr(\frac{1}{2}||x||_2^2 \leq ||\Phi x||_2^2 \leq \frac{3}{2}||x||_2^2 \geq 1 - 2*e^{-(1/4 -1/8)1000/2} \approx 1 - 5.4 * 10^{-14}$



\subsection*{10-5-2016}

$\Phi : \mathbb{R}^n \rightarrow \mathbb{R}^m, m < n$

$y = \Phi x, x \in \mathbb{R}^n, y \in \mathbb{R}^m$

x is s-sparse, when can we recover it from y?

2S-RIP

$(1-\delta)||x||_2^2 \leq ||\Phi x||_2^2 \leq (1+\delta)||x||_2^2$
for all 2s-sparse $x'$

The quantity $||\Phi x||_2^2$ is called concentration of measure.

$\Phi_{ij} ~ \mathcal{N}(0, 1/m)$

satisfaction of RIP when $m > const * S * log(n/s)$ with high probability

If we fix x then $(1-\delta)||x||_2^2 \leq ||\Phi x||_2^2 \leq (1+\delta)||x||_2^2$ holds with a probability  $p \geq  1 - 2 * e^{-(\frac{\delta^2}{2} + \frac{\delta^3}{3})m/2} = 1 - 2 e^{-C_0(\delta)m}$

Example:

Say $\delta = 1/2, m = 1000. Pr(\frac{1}{2}||x||_2^2 \leq ||\Phi x||_2^2 \leq \frac{3}{2}||x||_2^2 \geq 1 - 2*e^{-(1/4 -1/8)1000/2} \approx 1 - 5.4 * 10^{-14}$
$m=10,000, P > 1 - 4*10^{-136}$


Johnson-Lindenstrauss Lemma : Let $0 < \delta < 1$ be given.  For every set of points $Q_1(|Q| of points) \in \mathbb{R}^n$ if $m=O(\frac{log(|Q|/\epsilon}{\delta^2})$ then a randomly drawn $\Phi$ will satisfy this distortion: $(1-\delta)||u-v||_2^2 \leq ||\Phi u - \Phi v||_2^2 \leq (1+\delta)||u-v||_2^2, \forall$ pairs $u,v \in Q$ holds with probability of at least $1 - \epsilon$.

Note: Approximate query processing in databases.  You might not care so much if you didn't get the exact answer and instead got some approximate error.  Data streaming applications that use this are called sketching/streaming.  These are called JL-friendly distributions, examples:

Gaussian $\Phi_{ij} ~ \mathcal{N}(0, 1/m)$

Bernoulli $\Phi_{ij} = \frac{1}{\sqrt{m}}, -\frac{1}{\sqrt{m}}$ each with probability 1/2

Database friendly $\Phi_{ij} = \sqrt{\frac{3}{m}}, 0, -\sqrt{\frac{3}{m}}$ with probabilities 1/6, 2/3, 1/6 respectively.

Fast JL transform: $\Phi = PHD$, P: Sparse Gaussian, H: Fast Hadamard Transform, D: Random modulation matrix

We now show that CI implies 2S-RIP for $m~s * log(n/s)$.

Baraniuk Wakin DeVore Davenport (A Simple Proof of the Restricted Isometry Property for Random Matrices)
\url{http://yima.csl.illinois.edu/psfile/ECE598-08/JL_RIP.pdf}

Idea:  Fix T (support $x_1 |T| \leq 2S$) and see the probability of $||\Phi x|| \approx ||x||$ is very high.  Then count how many sets T exist to derive the worse case probability that RIP fails.

Lemma: $\Phi : \mathbb{R}^n \rightarrow \mathbb{R}^m$ be a random matrix with CI.  Let T be fixed of size $|T| \leq 2S < m, 0 < \delta < 1$ then we have (intentionally not squared) $(1-\delta)||x||_2 \leq ||\Phi x||_2 \leq (1+\delta)||x||_2 \forall x \in B_T$ (support on T, $||x|| =1$) with $p >  1 - 2(\frac{12}{\delta})^2s e^{-C_0(\delta/2)m}$

$B_T$ is a ball in 2S dimensional space

fill the sphere with balls of radius $\delta$ ex: $\delta = 0.2$ (covering number) $\mathbb{R}^d, \delta/4 \rightarrow (\frac{3}{\delta})^d$

\subsection*{10-10-2016}

$(1-\delta)||x||_2^2 \leq ||\Phi x||_2^2 \leq (1+\delta)||x||_2^2$ for all 2s-sparse $x$

Gaussian are 2S-RIP-$\delta$ with $m \geq c*S*log(n/s)$ (c is some constant)

Concentration Inequalities (CI): Fixed x, $pr(| ||\Phi x||^2 - ||x||^2| > \delta) < 2 e^{-C_0(\delta)m}$

Lemma: Let T be a fixed set $|T| \leq 2S < m$ and $0 < \delta < 1$

$(1-\delta)||x|| \leq ||\Phi x|| \leq (1+\delta)||x||, \forall x \in B_T$ with probability $\leq 1 - 2(\frac{12}{8})^{2S}e^{-C_0(\delta/2)m}$, $||x||_2^2 = 1$ x supported on T.

Proof: $B_T$ is a ball in 2S-dim space.  Choose a finite set Q that covers $B_T$ with radius $\delta/4$.  All $x \in B_T$ we have $\min_{q \in Q} ||x - q|| \leq \delta/4$

(From geometry: a cover needs $\frac{12}{\delta}^{2S} = |Q|$ points)

Concentration Inequality (CI) applied on each $q \in Q$ with $\delta/2$

$(1-\delta/2)||q||_2^2 \leq ||\Phi q||_2^2 \leq (1+\delta/2)||q||_2^2$, fails with probability $p < 2 e^{-C_0(\delta/2)m}$

Union Bound: If $A_1, A_2, ..., A_k$ are events $Pr(A_1 \cup ... \cup A_k) \leq \Sigma_{i=1}^k Pr(A_i)$

$A_i$ is $q_i$ failing the CI.  This means $(1-\delta/2)||q||_2^2 \leq ||\Phi q||_2^2 \leq (1+\delta/2)||q||_2^2$ holds for all $q \in Q$ with probability $ p > 1 - 2(\frac{12}{8})^{2S}e^{-C_0(\delta/2)m}$, $(1-\delta/2)||q|| \leq ||\Phi q|| \leq (1+\delta/2)||q||$ holds with the same probability.

Now define $A > ?$ be smallest number such that $||\Phi x|| \leq (1+A)||x||, \forall x \in B_T$, we want to show $A \leq \delta$ for any $x \in B_T$. pick $q \in Q$ such that $||x - q|| \leq \delta/4$, $||\Phi x|| = ||\Phi (q + x - q)|| \leq ||\Phi q|| + ||\Phi(x-q)|| \leq (1 + \delta/2) + (1 + A)||x-q|| \leq (1 + \delta/2) + (1+A) \delta/4 \leq ((1 + \delta/2) + (1+A) \delta/4)||x||$

$||\Phi x|| \leq (1 + \delta/2 + \delta/4 + A \delta/4)||x||$

$||\Phi x|| \leq (1 + A)||x||$


$1+A \leq 1 + \frac{3}{4} \delta + \frac{1}{4} \delta A$

$(1 - \frac{1}{4} \delta) A \leq \frac{3}{4} \delta $

$A \leq \frac{3\delta}{(4-\delta)} \leq \delta$


Show that for ALL 2S-sparse x $||\Phi x|| \approx ||x||$ with high probability.

Theorem: $\Phi : \mathbb{R}^n \rightarrow \mathbb{R}^m$ satisfying CI there exist constants $C_1, C_2, > 0$ depending on $\delta$ (only) such that RIP holds for desired $\delta$, when $m > C_1(\delta) S * log(n/s)$ with probability $p > 1 - 2e^{-C_0(\delta/2)m}$

Proof: We know for each $T(|T| \leq 2S)$ that $\Phi$ will fail with probability $p < 2(\frac{12}{8})^{2S}e^{-C_0(\delta/2)m}$.  There are ${n \choose 2S}$ sets of T exist.

(Taylor series note: $e^{2S} = \Sigma_{k=0}^\infty \frac{(2S)^k}{k!} > \frac{(2S)^{2S}}{(2S)!} \rightarrow \frac{1}{(2S)!} < \frac{e^{2S}}{(2S)^{2S}}$)

${n \choose 2S} = \frac{n!}{(2S)!(n-2S)!} = \frac{(n-2S+1)(n-2S+2)...n}{(2S)!} \leq \frac{n^{2S}}{(2S)!} < (\frac{n e}{2S})^{2S}$


Applying the union bound RIP will fail with probability $p \leq 2(\frac{en}{2S})^{2S}(\frac{12}{\delta})^{2S}e^{-C_0(\delta/2)m} = 2 exp(-C_0(\delta/2)m + 2S*log(\frac{2ne}{\delta 2 S}))$.  If you $m > C_1(\delta)s*log(n/s)$ then RIP holds with probability $p > 1 - 2 e^{-C_0(\delta)m}$ for some $C_2(\delta) \leq C_0(\delta/2) - \frac{1}{C_1(\delta)}( 1 + \frac{log(\frac{6ne}{\delta})}{log(n/s)}) \geq C_0(\delta/2) - \frac{2}{C_1(\delta)}$ when $n > \frac{6eS}{\delta}$

An $m \times n$ Gaussian matrix $\Phi$ obeys 2S-RIP-$\delta$ for $m \geq c * S * log(n/s)$

$\Rightarrow$ All s-sparse vectors can be recovered for $~c*S*log(n/s)$ measurements.

\subsection*{10-12-2016}

Orthogonal Matching Pursuit (OMP):

$x_{k+1} = x_k + <R_k, u_k> u_k / ||u_k||^2$

$R_{k+1} = R_l - <R_k, u_k> u_k / ||u_k||^2$

$[x]_N = D[a]_{2N}$

$||x - x_k^*|| \leq \frac{C^2}{2\beta -1}k^{-2\beta+1}$

$|a_1^*| \geq ... \geq |a_k^*|$


If $\Phi$ obeys a 2S-RIP-$\delta$

-Every s sparse x has an image y for which $\alpha$ is the unique pre-image

-Given y, one can recover x b solving $\min ||x||_0$ such that $\Phi x = y$, NP-Hard

If we relax $||x||_0$ to the next norm that we can optimize efficiently: $||x||_1$

$\min ||x||_1$ such that $\Phi x = y$ (Linear Programming)

Can we still recover the s-sparse x from this program?

(Candes, Romberg, and Tao)

\url{http://statweb.stanford.edu/~candes/papers/StableRecovery.pdf}

\url{http://statweb.stanford.edu/~candes/papers/DecodingLP.pdf}

Let $x_0$ be a vector in $\mathbb{R}^n$ supported on $T_0$, of $|T_0| \leq S$

We observe $ y = \Phi x_0$ and are interested in conditions under which $x_0$ is the EXACT solution to $\min ||x||_1$ such that $\Phi x = y$ (P1)

Let $x^*$ be the solution to P1, further more h denote the error in recovery: $h = x^* - x_0$

The error must satisfy two conditions:

1) $\Phi h = 0, h \in Null(\Phi)$, $\Phi x^* = y = \Phi x_0 \rightarrow \Phi (x^* - x_0) = 0$

2) $||x^*||_1 \leq ||x_0||_1 \rightarrow ||x_0 + h||_1 \leq ||x_0||_1$, Both are feasible, and $x^*$ is by definition the feasible point with smallest L1 norm.  There's only 1 point because it's convex.

Notation $h_T$ to be the restriction of h to the set T.

$h_T[i] = \{ h[i] (i \in T), 0 (i \not\in T)$

$||x_0 + h||_1 = \Sigma_{i \in T_0} |x_0[i] + h[i]| + \Sigma_{i \in T_0^c}|h[i]|$

triangle $\geq \Sigma_{i \in T_0} |x_0[i]| - |h[i]| + \Sigma_{i \in T_0^c}|h[i]|$

$= ||x_0||_1 - ||h_{T_0}||_1 + ||h_{T_0^C}||_1$

(2) tells us $||h_{T_0^C}||_1 \leq ||h_{T_0}||_1$


\subsection*{10-5-2016}

When we can solve $y = \Phi x$ for sparse solutions.  It's a requirement on $\Phi$

2S-RIP-$\delta$

$(1-\delta_S)||x||_2^2 \leq ||\Phi x||_2^2 \leq (1+\delta_S)||x||_2^2$ for all 2s-sparse $x'$.

$0 < \delta_S < 1 \rightarrow$ Every S-sparse vector can be recovered from y.


$(P_0) min_x ||x||_0$

$(P_1) min_x ||x||_1$

Candes, Romberg \& Tau

When $x^*$ happens to solve $(P_0)$

$h = x^* - x_0 \leftarrow$ exact, recovery $\rightarrow h = \vec{0}$, $x_0$ is a sparse vector

$x_0$ is a field s sparse vector $y = \Phi x_0$

Since both $x_0$ \& $x^*$ satisfy $\Phi x = y$

\begin{itemize}
\item $\Phi h = 0, h \in Null(\Phi)$
\item $||x^*||_1 < ||x_0||_1$
\item $||x_0 + h||_1 < ||x_0||_1$
\end{itemize}

$h_T[i] = \{ h[i] (i \in T), 0 (i \not\in T)$


$T_0 = \{2,4\}$

$T_0^C = \{1,3,5,6\}$

$x_0 = [0, 3, 0, -1, 0, 0]$

$h = [-, +, -, +, -, -]$

$x^* = [ ]$

$x_0 + h = x^*$

$h_{T_0} = [0, ?, 0, ?, 0, 0]$

$h_{T^C} = [?, 0, ?, 0, ?, ?]$

$h_{T_0} + h_{T_0^C} = h$

Derived the following from optimality: $||h_{T_0}||_1 > ||h_{T_0^C}||_1$

Show if $\Phi$ has RIP with particular $\delta$'s then

$\Phi h = 0 \Rightarrow ||h_{T_0}||_1 \leq \rho ||h_{T_0^C}||_1$, $|T_0| \leq S$, $\rho < 1$


Lemma: Let $\Phi : \mathbb{R}^n \rightarrow \mathbb{R}^m$ that obeys 3S-RIP $(\exists 0 < \delta_{3S} < 1)$

If $\delta_{2S} + 2 \delta_{3S} < 1$, then

$\forall h \in Null(\Phi) \rightarrow ||h_{T_0}||_1 < \rho ||h_{T_0^C}||_1$

For every $T_0$ with $|T_0| \leq S, \rho = \frac{1}{2} \frac{1 + \delta_{2S}}{1 -\delta_{3S}} < 1$

RIP makes use of $l_2$ norm of vectors

\begin{itemize}
\item Facts from LA: $\forall x \in \mathbb{R}^n, ||x||_1 \geq ||x||_2 \geq ||x||_\infty$
\item $||x||_1 \leq \sqrt{n} ||x||_2$
\item $||x||_2 \leq \sqrt{n} ||x||_\infty$
\item $||x||_1 \leq n ||x||_\infty$
\item $||h_{T_0}||_1 \leq \sqrt{s} ||h_{T_0}||_2$
\end{itemize}

\subsection*{10-19-2016}

$y = \Phi x, \mathbb{R}^m = \mathbb{R}^{m \times n} \mathbb{R}^n$

$s = 5, n = 10000, m = ?$

$\Phi$ is 2S-RIP $\rightarrow$ 10-RIP

$\exists \delta, 0 < \delta_{10} < 1$

$(1-\delta_{10})||x'||_2^2 \leq ||\Phi x'||_2^2 \leq (1+\delta_{10})||x'||_2^2$ for all 10-sparse $x'$.

$\Phi_{ij} \approx \mathcal{N}(0, \frac{1}{m})$

$m \geq c * 5 * \log(\frac{10000}{5})$

$m > 2s = 10$

$x^* = argmin||x||_1, \Phi x = y$

What conditions $x^* = x_0$

$\forall h \in NULL(\Phi), ||h_{T_0}||_1 \leq \rho ||h_{T_0^C}||_1$

Since $x^*$ is the optimal in (P1) $||h_{T_0^C}||_1 \leq ||h_{T_0}||_1$

$h = 0$ or $x^* = x_0$ (exact recovery)

$||h_{T_0}||_1 \leq \sqrt{S} ||h_{T_0}||_2$

\vspace{1cm}
Example

$x_0 = [0, 0, 5, -1, 0, 0]$

$T_0 = \{2, 4\}, (5,-1)$ from $x_0$

$T_0^C = \{0, 1, 3, 5, 6 \}, (0,0,0,0,0)$ from $x_0$

$x_0 + h = x^*$

$h_{T_0} =   \{0, 0, X, 0, X, 0, 0\}$

$h_{T_0^C} = \{X, X, 0, X, 0, X, X\}$

We divide $T_0^C$ to sets $T_1, T_2, ...$

$T_1 = $ location of the $s'$ biggest terms in $h_{T_0^C}$

$T_2 = $ location of the next $s'$ terms in $h_{T_0^C}$

$T_3 = $ ...

$\sin(\phi) \Phi h = 0 \rightarrow \Phi (h_{T_0 \cup T_1} + h_{(T_0 \cup T_1)^C}) = 0$

$\rightarrow \Phi h_{T_0 \cup T_1} = - \Sigma_{j \geq 2} \Phi h_{T_j}$

$\rightarrow ||\Phi h_{T_0 \cup T_1}||_2 = ||\Sigma_{j \geq 2} \Phi h_{T_j}|| \leq ||\Sigma_{j \geq 2} \Phi h_{T_j}||_2$

$h_{T_0 \cup T_1}$ has $S + S'$ non-zeros.  Invoke the RIP constraints for $S + S'$ sparsity level.

$\sqrt{1 - \delta_{S+ S'}} ||h_{T_0 \cup T_1}||_2 \leq ||\Phi h_{T_0 \cup T_1}||_2$

Also since each $h_{T_j}$ is $S'$ sparse.

$||\Phi h_{T_j}||_2 \leq \sqrt{1 + \delta_{S'}} ||h_{T_j}||_2$

$\Sigma_{j \geq 2} ||\Phi h_{T_j}||_2 \leq \sqrt{1 + \delta_{S'}} \Sigma_{j \geq 2}  ||h_{T_j}||_2$

Therefore:

$\sqrt{1 - \delta_{S+S'}}  ||h_{T_0 \cup T_1}||_2 \leq
 \sqrt{1 + \delta_{S'}}  ||h_{T_j}||_2$

For each $j \leq 2$, all of values in $h_{T_j}$ are less than ALL of the values in $h_{T_{j-1}}$

$\Rightarrow$ magnitudes in $h_{T_j}$ are less than the average of those in $h_{T_{j-1}}$

$||h_{T_{j}}||_\infty \leq \frac{1}{S'} ||h_{T_{j-1}}||_1$

Therefore: $||h_{T_{j}}||_2 \leq \sqrt{S'} ||h_{T_{j}}||_\infty \leq \frac{1}{\sqrt{S'}} ||h_{T_{j-1}}||_1$

$\Sigma_{j \geq 2}||h_{T_{j}}||_2 \leq \frac{1}{\sqrt{S'}} \Sigma_{j \geq 1} ||h_{T_{j}}||_1 = \frac{||h_{T_0^C}||_1}{\sqrt{S'}}$

$||h_{T_0 \cup T_1}||_2 \leq \sqrt{\frac{1+ \delta_{S'}}{1 - \delta_{S+S'}}} \frac{||h_{T_0^C||_1}}{\sqrt{S'}}$

$||h_{T_0}||_1 \leq \sqrt{S} ||h_{T_0}||_2 \leq \sqrt{S}||h_{T_0 \cup T_1}||_2$

define $\rho = \sqrt{\frac{S(1+\delta_{S'}}{S(1-\delta_{S+S'}}}$

$\sqrt{S}||h_{T_0 \cup T_1}||_2 < \rho ||h_{T_0^C}||_1$

How do we make  $\rho < 1$ ? $S' = 2S$. then $\rho^2 = \frac{1}{2}\frac{1 + \delta_{2S}}{1 - \delta_{3S}} < 1$

$1 + \delta_{2S} < 2 - 2\delta_{3S} \rightarrow \delta_{2S} + 2\delta_{3S} < 1$

$\delta_{2S} + 2\delta_{3S} < 1$

$\delta_{2S} < \delta_{3S}$

$\rightarrow 3 \delta_{2S} < 1 \rightarrow \delta_{3S} < \frac{1}{3} \rightarrow h = 0$

$y = \Phi x, \delta_{2S} < \sqrt{2} - 1$

\subsection*{10-21-2016}

If you find code available, you need to acknowledge it.  Maybe you can break the algorithm that they present.  This involves understanding it to see what sort of data will make it less attractive.  Find a problem you're interested in so that most of the time is pleasurable.  How much have you learned and extended your knowledge beyond class?  How much you can make a good presentation in educating others?  These two questions should be on your mind when going through tasks.

Sparse solution to $y = \Phi x$

$x_0 =  argmin_x ||x||_0, \Phi x = y$

$x^* =  argmin_x ||x||_1, \Phi x = y$, L1 minimization, basis pursuit

Exact recovery $h = x^* - x_0 = \vec{0}$ what conditions?

If $\Phi$ is a 3S-RIP matrix, condition $\delta_{2S} + 2\delta_{3S} < 1 \Rightarrow h = \vec{0}$

If $\Phi$ is a 2S-RIP matrix, condition $\delta_{2S} < \sqrt{2} -1 \Rightarrow h = \vec{0}$

\begin{enumerate}
\item $x^*$ is $l_1$ optimal, $||h_{T_0}||_1 \geq ||h_{T_0^C}||_1$
\item In order to ensure $h = \vec{0}$, $||h_{T_0}||_1 \leq \rho||h_{T_0^C}||_1, \exists \rho < 1, \forall h \in Null(\Phi), \Phi h = 0$ For every set $T_0, |T_0| \leq S$  How?  If $S_{2S} < \sqrt{2} - 1$ Null Space Property of orders (NSP), means you can't have sparse vectors in the null space of $\Phi$, they have to be more or less well balanced, can't find a small subset where the energy is located.
\end{enumerate}

$l_0 - l_1$ equivalence needs NSP.

Another way of establishing conditions on $\Phi$ that imply $l_0 - l_1$ equivalence, in other words $x^* = x_0, h = \vec{0}$

RIP constants are more or less applicable in the random world, other types of matrices benefit from this condition.  There are other conditions that you can satisfy.  $x^*$ (l1 minimization) is practical, $x_0$ is theoretical. $||x_0 + h||_1 = \Sigma_{i \in T_0} |x_0[i] + h[i]| + \Sigma_{i \in T_0^C} |h[i]| \geq \Sigma_{i \in T_0} |x_0[i]| - |h[i]| + \Sigma_{i \in T_0^C} |h[i]| = ||x_0||_1 + ||h||_1 - 2||h_{T_0}||_1 \geq ||x_0||_1 + ||h||_1 - 2\sqrt{S}||h_{T_0}||_2 \geq ||x_0||_1 + ||h||_1 - 2\sqrt{S}||h||_2 \Rightarrow$ $l_1$-min is exact if $\frac{||h||_1}{||h||_2} > 2 \sqrt{S}, \forall h \in Null(\Phi)$ called the spherical section property(SSP)

Always($\forall h$): $1 \leq \frac{||h||_1}{||h||_2} \leq \sqrt{n}$


Theorem: [Kashin, Granarev, Gluskin] A randomly drawn $n-m$ dimensional subspace, H, of $\mathbb{R}^n$ satisfies $\frac{||h||_1}{||h||_2} \leq \frac{c_1 \sqrt{m}}{\sqrt{1 + log(n/m)}}, \forall	 h \in H$ with probability at least $1 - exp(-c_0(n-m))$ with $c_0, c_1$ constants.

The establishment of equivalence between $l_0$ and $l_1$ is what makes this theory practical.  You want the sparsest solutions when doing pattern recognition or dealing with MRI, etc.

Stable Recovery: So far we have assumed there was a sparse solution to our linear system.  What if we don't know that?  What if it isn't exactly sparse?  How will it react to that failure to guarantee sparsity?  If $x_0$ is not sparse, can we hope for graceful failure?  If you decrease your sampling rate just below the nyquist frequency, it destroys your image entirely.  If you're using $l_1$ minimization you won't get anything as bad as in the classical world with aliasing.  With these RIP conditions the $l_1$ minimization will find an optimal point $x^*$ that is close or similar to the best S-term approximation.  If $x_0$ is a generic vector which is not sparse, Let $x_{0,S}$ denote a vector with only top S-terms in $x_0$ (there $s \neq \emptyset$)  Since $x_0$ is not sparse, $x^* \neq	 x_0$, but (error from $l_1$-min) $||x^* - x_0||_1 \leq C_\delta ||x_0 - x_{0,S}||_1$, C is constant depending on RIP only, the last norm is the error in best s-term approximation.  $||x^* - x_0||_2 \leq C_\delta \sqrt{S}||x_0 - x_{0,S}||_1$

\subsection*{10-21-2016}

.

P(0),$\min_x||x||_0, \Phi x = y$

P(1),$\min_x||x||_1, \Phi x = y$

$l_0 - l_1$ equivalence

If we can guarantee $\forall h \in Null(\Phi)$

(*), $||h_{T_0}||_1 \leq \rho || h_{T_0^C}||_1, \rho < 1$

RIP: $\delta_{2S} + 2\delta{3S} < 1$, (*) is satisfied.

Spherical section property (SSP)

$\frac{||h||_1}{||h||_2} \geq 2 \sqrt{S}$

Let $S = 2$

Ex, h cannot be $[0, 0, 1, 2, 0, 0, -2]$, $T_0 = \{3,4,7\}, ||h_{T_0}||_1 = 5, ||h_{T_0^C}|| = 0$

Stable Recovery (again): $x_0$ is no longer sparse.  Let $x^* = argmin ||x||_1 s.t. \Phi x = y$.  $x^*$ can not be expected to be $x_0$ since the latter is not sparse.  But, we are going to show that $||x^* - x_0|| \leq C_\delta ||x_0 - x_{0,S}||_1$, $C_\delta$ are the RIP constants.  As before, the next part is the error in best s-term approximation to $x_0$.

The key idea here is to tweak the optimality inequality $||h_{T_0^C}|| \leq ||h_{T_0}||_1$ for the case $x_0$ is not sparse.  $h = x^* - x_0$, $T_0$ is the location of S largest elements of $x_0$ then $||x^*||_1 = ||x_0 + h||_1 = \Sigma_{i \in T_0} |x_0[i] + h[i]| + \Sigma_{i \in T_0^C} |x_0[i] + h[i]| \geq \Sigma_{i \in T_0} |x_0[i]| - |h[i]| + \Sigma_{i \in T_0^C} |h[i]| - |x_0[i]| = ||x_0||_1 - 2\Sigma_{i \in T_0^C} |x[i]| - ||h_{T_0}||_1 + ||h_{T_0^C}||_1 = ||x_0||_1 - 2\Sigma_{i \in T_0^C}|x[i]| - ||h_{T_0}||_1 + ||h_{T_0^C}||_1 = ||x_0||_1 - ||h_{T_0}||_1 + ||h_{T_0^C}||_1 - 2||x_0 - x_{0,S}||_1$.  We know $||x_0 + h||_1 \leq ||x_0||_1$ (because $x_0 + h = x^*$ is optimal)

$||h_{T_0^C}||_1 < ||h_{T_0}||_1 + 2||x_0 - x_{0,S}||_1$

We have seen before, that if $\Phi$ obeys the 3S-RIP then $\exists \rho < 1$ (depends on RIP constants) such that:

$\forall h \in Null(\Phi), |h_{T_0}||_1 \leq \rho |h_{T_0^C}||_1, \forall T_0, |T_0| \leq S$

Therefore $||h_{T_0^C}||_1 \leq \frac{2}{1-\rho} ||x_0 - x_{0,S}||_1$

and $||x^* - x_0||_1 = ||h||_1 = ||h_{T_0}||_1 + ||h_{T_0^C}||_1$

$\leq (1 + \rho) ||h_{T_0^C}||_1 \leq \frac{2(1+\rho)}{1-\rho} ||x_0 - x_{0,S}||_1$

We don't care whether $x_0$ is sparse or not, we just plug it into $l_1$ minimization we get $x^*$ which is as close as we can get with s terms.  Blindly using $l_1$ minimization the $x^*$ is close to the true solution (what you really are trying to find), as close as the s-term approximation gets.  If you want 2S, you can rewind and see what RIP constant on $\Phi$ would give you the best n-term approximation.  We want to perform as few measurements as possible, that means $\Phi$ has the smallest number of rows possible.


Stable Recovery: $l_2$ norm, $||x^* - x_0||_2 \leq C_\delta \frac{||x_0 - x_{0,S}||_1}{\sqrt{S}}$


\end{document}
